---
title: 2025.1.9 学习内容
tags: Record
---

《机器学习理论引导》第7章

​&#8226; 基本概念（迭代优化算法，收敛率和迭代复杂度等价衡量算法性能）

​&#8226; 确定优化：梯度下降

  ​1.一般凸优化问题（凸函数）-收敛率 $O(1/\sqrt{T})$；

  ​2.光滑强凸函数（任意点可构造一个二次函数作为上界）-收敛率 $O(1/{\beta}^T)$ 其中$\beta$>1 .

​&#8226; 随机优化：随机梯度下降（相比于梯度下降，其不需要计算每一步$\nabla f(\omega)$的真实值，而是通过随机采样获得一个无偏估计值$g_t$）

  ​1.凸函数-收敛率 $O(1/\sqrt{T})$；

  ​2.$\lambda$强凸函数-阶段随机梯度下降-收敛率 $O(1/[{\lambda}T])$；

ps. 随机优化（SGD）每次迭代只需要处理一个或少量样本数据，适用于大规模数据集。随机优化里面的证明有点复杂，得再多看几遍，自己推导一下。
